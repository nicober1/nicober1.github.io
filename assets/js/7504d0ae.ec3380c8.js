"use strict";(self.webpackChunkfluentblogs=self.webpackChunkfluentblogs||[]).push([[9466],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>h});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},d=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),u=c(a),p=i,h=u["".concat(l,".").concat(p)]||u[p]||m[p]||o;return a?n.createElement(h,r(r({ref:t},d),{},{components:a})):n.createElement(h,r({ref:t},d))}));function h(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,r=new Array(o);r[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:i,r[1]=s;for(var c=2;c<o;c++)r[c]=a[c];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},15450:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>c,toc:()=>u});var n=a(87462),i=a(63366),o=(a(67294),a(3905)),r=["components"],s={keywords:["GPT-3","GPT-4","OpenAI","Azure OpenAI"]},l=void 0,c={unversionedId:"Azure OpenAI GPT-3 GPT-4",id:"Azure OpenAI GPT-3 GPT-4",title:"Azure OpenAI GPT-3 GPT-4",description:"- AI Democratization",source:"@site/docs/Azure OpenAI GPT-3 GPT-4.md",sourceDirName:".",slug:"/Azure OpenAI GPT-3 GPT-4",permalink:"/docs/Azure OpenAI GPT-3 GPT-4",draft:!1,tags:[],version:"current",frontMatter:{keywords:["GPT-3","GPT-4","OpenAI","Azure OpenAI"]},sidebar:"myAutogeneratedSidebar",previous:{title:"Azure DevOps Pipeline",permalink:"/docs/Azure DevOps Pipeline"},next:{title:"Azure Powershell",permalink:"/docs/Azure Powershell"}},d={},u=[{value:"Selective Context",id:"selective-context",level:3},{value:"What exactly is GPT-3?",id:"what-exactly-is-gpt-3",level:2},{value:"Different kinds of prompts",id:"different-kinds-of-prompts",level:3},{value:"Semantic search",id:"semantic-search",level:3},{value:"Moderation",id:"moderation",level:3},{value:"Embeddings",id:"embeddings",level:3},{value:"Examples of Weak AI",id:"examples-of-weak-ai",level:3}],m={toc:u},p="wrapper";function h(e){var t=e.components,a=(0,i.Z)(e,r);return(0,o.kt)(p,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"AI Democratization")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Azure Cognitive Search is one of these knowledge mining solutions.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"The following are the six principles of responsible AI: \u2022 Fairness \u2022 Reliability and safety \u2022 Privacy and security \u2022 Inclusiveness \u2022 Transparency \u2022 Accountability")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Azure Cognitive Services are a type of artificial intelligence (AI) service that is hosted in the cloud. With these services, software developers can add cognitive features to their apps without having to be experts in AI or data science. Both client library software development kits (SDKs) and REST application programming interfaces (APIs) make it possible to access them in a wide range of commonly used programming languages. By using cognitive solutions that can see, hear, speak, and analyze data, Chapter 2 Fundamentals of Artificial Intelligence 26 Azure Cognitive Services makes it easy for developers to add cognitive capabilities to their apps. These cognitive solutions can see, hear, speak, and analyze data. The following are the categories of Azure Cognitive Services: \u2022 Vision \u2022 Computer Vision \u2022 Custom Vision \u2022 Face \u2022 Speech \u2022 Speech service \u2022 Language \u2022 Language Service \u2022 Translator \u2022 Language Understanding (LUIS) \u2022 QnA Maker \u2022 Decision \u2022 Anomaly detector \u2022 Content moderator \u2022 Personalizer")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"The essentials of machine learning can be found in all the following algorithms: \u2022 Linear regression \u2022 Logistic regression \u2022 CART (Classification and Regression Trees) \u2022 Na\xefve Bayes \u2022 KNN (K-Nearest Neighbors) \u2022 Apriori \u2022 K-means \u2022 PCA (Principal Component Analysis) \u2022 Bagging with random forests \u2022 Boosting with AdaBoost")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Exquisite language models like GPT-3, Codex, and Embeddings are made easily available through REST APIs in OpenAI.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Cosine similarity is a way of measuring how similar two vectors are. It looks at the angle between two vectors (lines) and compares them. Cosine similarity is the cosine of the angle between the vector. A result is a number between -1 and 1. If the vectors are the same, the result is 1. If the vectors are completely different, the result is -1. If the vectors are at a 90-degree angle, the result is 0.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"A better solution is using a fuzzy search technique. For example, we can use the Levenshtein distance\xb2\u2076 technique using Python. In simple words, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions, or substitutions) needed to transform one word into the other. You don\u2019t need to reimplement any algorithm by yourself, as most of them can be found in libraries like textdistance.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"The capability of \u201cfew-shot learning\u201d allows GPT-3 to quickly comprehend instructions provided to it, even with minimal data. In other words, GPT-3 can be programmed to complete tasks with only a few examples as input. This opens up a new world of limitless possibilities for AI-driven applications and domains.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"JSONL, also known as newline-delimited JSON, is a useful format for storing structured data that can be processed one record at a time."))),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json"},'\n{"prompt":"When do I have to start the heater?", "completion":"Every day in the morn\\\ning at 7AM. You should stop it at 2PM"}\n{"prompt":"Where is the garage remote control?", "completion":"Next to the yellow do\\\nor, on the key ring"}\n{"prompt":"Is it necessary to program the scent diffuser every day?", "completion":"\\\nThe scent diffuser is already programmed, you just need to recharge it when its batt\\\nery is low"}\n\n\n')),(0,o.kt)("h3",{id:"selective-context"},"Selective Context"),(0,o.kt)("p",null,"An initial prompt is saved to a text file \u2022 The user enters a prompt \u2022 The program creates embeddings for all interactions in the file \u2022 The program creates embeddings for the user\u2019s prompt \u2022 The program calculates the cosine similarity between the user\u2019s prompt and all interactions in the file \u2022 The program sorts the file by the cosine similarity \u2022 The best n interactions are read from the file and sent with the prompt to the user"),(0,o.kt)("p",null,"Generative Pre-trained Transformer Version 3 (GPT-3)"),(0,o.kt)("p",null,"Artificial Intelligence (AI)"),(0,o.kt)("p",null,"Natural Language Processing (NLP)."),(0,o.kt)("p",null,"In short, GPT-3 is a language model: a statistical model that calculates the probability distribution over a sequence of words. In other words, GPT-3 is a system for guessing which text comes next when text is given as an input."),(0,o.kt)("h2",{id:"what-exactly-is-gpt-3"},"What exactly is GPT-3?"),(0,o.kt)("p",null,"Although GPT-3 is a general-purpose NLP system, it really just does one thing: it predicts what comes next based on the text that is provided as input. But it turns out that, with the right architecture and enough data, this one thing can handle a stunning array of language processing tasks. GPT-3 is the third version of the GPT language model from OpenAI. So, although it started to become popular in the summer of 2020, the first version of GPT was announced 2 years earlier, and the following version, GPT-2, was announced in February 2019. But even though GPT-3 is the third version, the general system design and architecture hasn't changed much from GPT-2. There is one big difference, however, and that's the size of the dataset that was used for training. GPT-3 was trained with a massive dataset comprised of text from the internet, books, and other sources, containing roughly 57 billion words and 175 billion parameters. That's 10 times larger than GPT-2 and the next-largest language model. To put the model size into perspective, the average human might read, write, speak, and hear upward of a billion words in an entire lifetime. So, GPT-3 has been trained on an estimated 57 times the number of words most humans will ever process. The GPT-3 language model is massive, so it isn't something you'll be downloading and dabbling with on your laptop. But even if you could (which you can't because it's not available to download), it would cost millions of dollars in computing resources each time you wanted to build the model. This would put GPT-3 out of reach for most small companies and virtually all individuals if you had to rely on your own computer resource to use it. Thankfully, you don't. OpenAI makes GPT-3 available through an API that is both affordable and easy to use. So, anyone can use some of the most advanced AI ever created!"),(0,o.kt)("p",null,"The text you pass in is referred to as a prompt, and the returned text is called a completion. A prompt is used by GPT-3 to determine how best to complete the task. In the simplest case, a prompt can provide a few words to get started with. For example, if the prompt was If today is Monday, tomorrow is, GPT-3 would likely respond with Tuesday, along with some additional text such as If today is Tuesday, tomorrow is Wednesday, and so on. This means that what you get out of GPT-3 depends on what you send to it."),(0,o.kt)("h3",{id:"different-kinds-of-prompts"},"Different kinds of prompts"),(0,o.kt)("p",null,"\u2022 Zero-shot prompts \u2022 One-shot prompts \u2022 Few-shot prompts"),(0,o.kt)("p",null,"Davinci is the most capable model and can do anything that any other model can do, and much more\u2014often with fewer instructions. Davinci is able to solve logic problems, determine cause and effect, understand the intent of text, produce creative content, explain character motives, and handle complex summarization tasks."),(0,o.kt)("p",null,"Curie Curie tries to balance power and speed. It can do anything that Ada or Babbage can do but it's also capable of handling more complex classification tasks and more nuanced tasks like summarization, sentiment analysis, chatbot applications, and Question and Answers."),(0,o.kt)("p",null,"Babbage Babbage is a bit more capable than Ada but not quite as performant. It can perform all the same tasks as Ada, but it can also handle a bit more involved classification tasks, and it's well suited for semantic search tasks that rank how well documents match a search query."),(0,o.kt)("p",null,"Ada Ada is usually the fastest model and least costly. It's best for less nuanced tasks\u2014for example, parsing text, reformatting text, and simpler classification tasks. The more context you provide Ada, the better it will likely perform."),(0,o.kt)("h3",{id:"semantic-search"},"Semantic search"),(0,o.kt)("p",null,"A key component of semantic search is the use of embedding, which is the process of representing words or phrases as numerical vectors. These vectors are generated by a neural network that analyzes the context of each word or phrase in a given text corpus. By converting words into vectors, it becomes easier to measure the semantic similarity between words and phrases, which is crucial for accurate search results"),(0,o.kt)("h3",{id:"moderation"},"Moderation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Moderation is a fine-tuned model developed by OpenAI that can detect potentially sensitive or unsafe text content. Moderation uses ML algorithms to classify text as safe or unsafe based on its context and language use. This model can be used to automate content moderation on social media platforms, online communities, and in many other domains. There are multiple categories, such as hate, hate/threatening, self-harm, sexual, sexual/minors, violence, violence/graphic.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Encoders are layers that transform natural language input into numerical vectors. This is achieved thanks to the process of embedding, an NLP technique that represents words with vectors in such a way that once represented in a vectorial space, the mathematical distance between vectors is representative of the similarity among words they represent."))),(0,o.kt)("h3",{id:"embeddings"},"Embeddings"),(0,o.kt)("p",null,"Some models can use embeddings. These embeddings involve representing words or sentences in a multi-dimensional space. The mathematical distances between different instances in this space represent their similarity in terms of meaning. As an example, imagine the words queen, woman, king, and man. Ideally, in our multidimensional space, where words are vectors, if the representation is correct, we want to achieve the following: This means that the distance between woman and man should be equal to the distance between queen and king."),(0,o.kt)("p",null,"Embeddings can be extremely useful in intelligent search scenarios. Indeed, by getting the embedding of the user input and the documents the user wants to search, it is possible to compute distance metrics (namely, cosine similarity) between the input and the documents. By doing so, we can retrieve the documents that are closer, in mathematical distance terms, to the user input."),(0,o.kt)("h3",{id:"examples-of-weak-ai"},"Examples of Weak AI"),(0,o.kt)("p",null,"\u2022 Google Maps \u2022 Apple autocorrect \u2022 Chatbots \u2022 Smart assistants such as Siri, Alexa, and Cortana"))}h.isMDXComponent=!0}}]);